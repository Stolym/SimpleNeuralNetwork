{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do you make your personnal neural network ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, params):\n",
    "        #Constructor class, you must set params, where you can find layers and activations\n",
    "        #First step mount layers Weight && Bias\n",
    "        self.mountLayer(params)\n",
    "        self.losses = []\n",
    "        self.cost = []\n",
    "        self.accurency = []\n",
    "\n",
    "    def mountLayer(self, params):\n",
    "        #Initalize layer object\n",
    "        self.layer={}\n",
    "        #Save activation in class\n",
    "        self.activations = params[\"activation\"]\n",
    "        #Initialize all weights and bias\n",
    "        #For the weight, we use the random beetween -1 1 it's better to learn fast !\n",
    "        #We use float64 because if you don't normalize your data, you can get overflow...\n",
    "        for i in range(1, len(params[\"size\"])):\n",
    "            self.layer[\"W\"+str(i)] = np.array(np.random.rand(params[\"size\"][i], params[\"size\"][i - 1]), dtype=np.float64) * 2 - 1\n",
    "            self.layer[\"B\"+str(i)] = np.array(np.zeros((params[\"size\"][i],1)), dtype=np.float64)\n",
    "\n",
    "    def backpropLayer(self, i):\n",
    "        _DA = self.layer[\"DA\"+str(i)]\n",
    "        W = self.layer[\"W\"+str(i)]\n",
    "        Z = self.layer[\"Z\"+str(i)]\n",
    "        _A = self.layer[\"A\"+str(i - 1)]\n",
    "        activation =  self.activations[i]\n",
    "        if (activation == \"relu\"):\n",
    "            self.layer[\"DZ\"+str(i)] = self.derivate_relu(_DA, Z)\n",
    "            self.layer[\"DW\"+str(i)] = np.dot(self.layer[\"DZ\"+str(i)], _A.T)/_A.shape[0]\n",
    "            self.layer[\"DB\"+str(i)] = np.sum(self.layer[\"DZ\"+str(i)], axis=1, keepdims=True)/_A.shape[0]\n",
    "            self.layer[\"DA\"+str(i - 1)] = np.dot(W.T, self.layer[\"DZ\"+str(i)])\n",
    "        elif (activation == \"sigmoid\"):\n",
    "            self.layer[\"DZ\"+str(i)] = self.derivate_sigmoid(_DA, Z)\n",
    "            self.layer[\"DW\"+str(i)] = np.dot(self.layer[\"DZ\"+str(i)], _A.T)/_A.shape[0]\n",
    "            self.layer[\"DB\"+str(i)] = np.sum(self.layer[\"DZ\"+str(i)], axis=1, keepdims=True)/_A.shape[0]\n",
    "            self.layer[\"DA\"+str(i - 1)] = np.dot(W.T, self.layer[\"DZ\"+str(i)])\n",
    "        elif (activation == \"tanh\"):\n",
    "            self.layer[\"DZ\"+str(i)] = self.derivate_tanh(_DA, Z)\n",
    "            self.layer[\"DW\"+str(i)] = np.dot(self.layer[\"DZ\"+str(i)], _A.T)/_A.shape[0]\n",
    "            self.layer[\"DB\"+str(i)] = np.sum(self.layer[\"DZ\"+str(i)], axis=1, keepdims=True)/_A.shape[0]\n",
    "            self.layer[\"DA\"+str(i - 1)] = np.dot(W.T, self.layer[\"DZ\"+str(i)])\n",
    "        #elif (activation == \"swish\"):\n",
    "        #    self.layer[\"DZ\"+str(i)] = self.derivate_swish(_DA, Z)\n",
    "        #    self.layer[\"DW\"+str(i)] = np.dot(self.layer[\"DZ\"+str(i)], _A.T)/_A.shape[0]\n",
    "        #    self.layer[\"DB\"+str(i)] = np.sum(self.layer[\"DZ\"+str(i)], axis=1, keepdims=True)/_A.shape[0]\n",
    "        #    self.layer[\"DA\"+str(i - 1)] = np.dot(W.T, self.layer[\"DZ\"+str(i)])\n",
    "        #elif (activation == \"leaky_relu\"):\n",
    "        #    self.layer[\"DZ\"+str(i)] = self.derivate_leaky_relu(_DA, Z)\n",
    "        #    self.layer[\"DW\"+str(i)] = np.dot(self.layer[\"DZ\"+str(i)], _A.T)/_A.shape[0]\n",
    "        #    self.layer[\"DB\"+str(i)] = np.sum(self.layer[\"DZ\"+str(i)], axis=1, keepdims=True)/_A.shape[0]\n",
    "        #    self.layer[\"DA\"+str(i - 1)] = np.dot(W.T, self.layer[\"DZ\"+str(i)])\n",
    "        #elif (activation == \"mish\"):\n",
    "        #    self.layer[\"DZ\"+str(i)] = self.derivate_mish(_DA, Z)\n",
    "        #    self.layer[\"DW\"+str(i)] = np.dot(self.layer[\"DZ\"+str(i)], _A.T)/_A.shape[0]\n",
    "        #    self.layer[\"DB\"+str(i)] = np.sum(self.layer[\"DZ\"+str(i)], axis=1, keepdims=True)/_A.shape[0]\n",
    "        #    self.layer[\"DA\"+str(i - 1)] = np.dot(W.T, self.layer[\"DZ\"+str(i)])\n",
    "\n",
    "            \n",
    "    # Here all derivate functions\n",
    "    \n",
    "    def derivate_tanh(self, _DA, x):\n",
    "        return _DA * (1 - np.tanh(x)**2)\n",
    "\n",
    "    def derivate_relu(self, _DA, x):\n",
    "        # Copy matrix _DA in DZ, why ? if not the dimensions don't match with next matrix..\n",
    "        DZ = np.array(_DA, copy = True, dtype=np.float64)\n",
    "        DZ[x < 0] = 0\n",
    "        return DZ\n",
    "\n",
    "    def derivate_sigmoid(self, _DA, x):\n",
    "        return _DA * (self.sigmoid(x)*(1-self.sigmoid(x)))\n",
    "\n",
    "    def derivate_leaky_relu(self, _DA, x):\n",
    "        # Copy matrix _DA in DZ, why ? if not the dimensions don't match with next matrix..\n",
    "        DZ = np.array(_DA, copy = True, dtype=np.float64)\n",
    "        DZ[x < 0] = DZ[x < 0] * 0.01\n",
    "        return DZ\n",
    "    \n",
    "    \n",
    "    #Armand you are welcome to help me here /!\\\n",
    "    def derivate_swish(self, _DA, x):\n",
    "        pass\n",
    "    \n",
    "    #def derivate_mish(self, _DA, x):\n",
    "    #    omega = np.exp(3 * x) + 4 * np.exp(2 * x) + (6 + 4 * x) * np.exp(x) + 4 * (1 + x)\n",
    "    #    delta = 1 + np.pow((np.exp(x) + 1), 2)\n",
    "    #    derivative = np.exp(x) * omega / np.pow(delta, 2)\n",
    "    #    return derivative * _DA\n",
    "    \n",
    "    def derivate_mish(self, _DA, x):\n",
    "        pass\n",
    "    \n",
    "    #-----------------------------------------------------\n",
    "    \n",
    "    #Cost function but i don't use it now because exemple is not very difficult\n",
    "    def compute_cost(self, Y, A):\n",
    "        m = Y.shape[1]\n",
    "        logprobs = np.multiply(np.log(A), Y) + np.multiply(1 - Y, np.log(1 - A))\n",
    "        cost = - np.sum(logprobs) / m\n",
    "        cost = np.squeeze(cost)\n",
    "        return cost\n",
    "\n",
    "    #If you need only prediction, after train\n",
    "    def predict(self, input):\n",
    "        self.layer[\"A0\"] = input\n",
    "        for i in range(1, len(self.activations)):\n",
    "            self.forwardLayer(i)\n",
    "        return self.layer[\"A\"+str(len(self.activations)-1)]\n",
    "\n",
    "    def train(self, input, output, epoch):\n",
    "        for _ in range(epoch):\n",
    "            #Set Input\n",
    "            self.layer[\"A0\"] = input\n",
    "\n",
    "            #Forward activation\n",
    "            for i in range(1, len(self.activations)):\n",
    "                self.forwardLayer(i)\n",
    "\n",
    "            #Cost\n",
    "            self.layer[\"COST\"] = np.subtract(output, self.layer[\"A\"+str(len(self.activations)-1)])\n",
    "            #Loss\n",
    "            self.layer[\"LOSS\"] = np.abs(np.subtract(output, self.layer[\"A\"+str(len(self.activations)-1)]))\n",
    "            \n",
    "            #You can remove this part if you don't need to see it's progression with matplot\n",
    "            self.losses.append(np.sum(self.layer[\"LOSS\"]))\n",
    "            self.accurency.append(1 - np.mean(self.layer[\"LOSS\"]))\n",
    "            self.cost.append(np.sum(np.abs(self.layer[\"COST\"])))\n",
    "            #----------------------------------------------------\n",
    "            \n",
    "            \n",
    "            #self.layer[\"DA\" + str(len(self.activations) - 1)] = np.subtract(output, self.layer[\"A\"+str(len(self.activations)-1)]) * self.sigmoid(self.layer[\"A\"+str(len(self.activations)-1)])*(1-self.sigmoid(self.layer[\"A\"+str(len(self.activations)-1)]))\n",
    "            \n",
    "            #Derivate of last activation, I use divide because it's better to learn fast\n",
    "            self.layer[\"DA\"+str(len(self.activations) - 1)] =- (np.divide(output, self.layer[\"A\"+str(len(self.activations)-1)]) - np.divide(1 - output, 1 - self.layer[\"A\"+str(len(self.activations)-1)]))\n",
    "            \n",
    "            #Back propagation\n",
    "            for i in range(len(self.activations) - 1, 0, -1):\n",
    "                self.backpropLayer(i)\n",
    "            \n",
    "            #Learn part\n",
    "            self.learn()\n",
    "\n",
    "    def learn(self):\n",
    "        #Here you can adjust your learning rate, actual 1e-2\n",
    "        for i in range(1, len(self.activations)):\n",
    "            self.layer[\"W\"+str(i)] -= self.layer[\"DW\"+str(i)] * 1e-2\n",
    "            self.layer[\"B\"+str(i)] -= self.layer[\"DB\"+str(i)] * 1e-2\n",
    "\n",
    "\n",
    "    def forwardLayer(self, i):\n",
    "        W = self.layer[\"W\" + str(i)]\n",
    "        B = self.layer[\"B\" + str(i)]\n",
    "        _A = self.layer[\"A\"+ str(i - 1)]\n",
    "        activation = self.activations[i]\n",
    "\n",
    "        if (activation == \"relu\"):\n",
    "            self.layer['Z' + str(i)] = np.dot(W, _A) + B\n",
    "            self.layer['A' + str(i)] = self.relu(self.layer[\"Z\" + str(i)])\n",
    "        elif (activation == \"sigmoid\"):\n",
    "            self.layer['Z' + str(i)] = np.dot(W, _A) + B\n",
    "            self.layer['A' + str(i)] = self.sigmoid(self.layer[\"Z\" + str(i)])\n",
    "        elif (activation == \"tanh\"):\n",
    "            self.layer['Z' + str(i)] = np.dot(W, _A) + B\n",
    "            self.layer['A' + str(i)] = np.tanh(self.layer[\"Z\" + str(i)])\n",
    "        #elif (activation == \"swish\"):\n",
    "        #    self.layer['Z' + str(i)] = np.dot(W, _A) + B\n",
    "        #    self.layer['A' + str(i)] = np.swish(self.layer[\"Z\" + str(i)])\n",
    "        #elif (activation == \"leaky_relu\"):\n",
    "        #    self.layer['Z' + str(i)] = np.dot(W, _A) + B\n",
    "        #    self.layer['A' + str(i)] = np.leaky_relu(self.layer[\"Z\" + str(i)])\n",
    "        #elif (activation == \"mish\"):\n",
    "        #    self.layer['Z' + str(i)] = np.dot(W, _A) + B\n",
    "        #    self.layer['A' + str(i)] = self.mish(self.layer[\"Z\" + str(i)])\n",
    "\n",
    "\n",
    "    def softplus(self, x):\n",
    "        return np.log(1 + np.exp(x))\n",
    "    \n",
    "    # All activation functions\n",
    "\n",
    "    #def mish(self, x):\n",
    "    #    return x * np.tanh(self.softplus(x))\n",
    "\n",
    "    #def new_mish(self, x):\n",
    "    #    return x * (np.exp(self.softplus(x)) - np.exp(-self.softplus(x))) / (np.exp(self.softplus(x)) + np.exp(-self.softplus(x)))\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        return 1/(1 + np.exp(-x))\n",
    "\n",
    "    def relu(self, x):\n",
    "        x[x < 0] = 0\n",
    "        return x\n",
    "    \n",
    "    def swish(self, x):\n",
    "        pass\n",
    "    \n",
    "    #def leaky_relu(self, x):\n",
    "    #    x[x < 0] = x[x < 0] * 0.01\n",
    "    #    return x\n",
    "    #---------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After to learn how this class work, let's go to test it ! :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:25: RuntimeWarning: divide by zero encountered in true_divide\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:25: RuntimeWarning: invalid value encountered in true_divide\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:34: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAD4CAYAAADhNOGaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAOpklEQVR4nO3cf6jd9X3H8eeruTRrEUyi8UeN2bVVGHGDFg5K2QauaoyDNtL6h90fDVtL/lj9Y5VCUxzT2v6hbp2ltNsIbSEIa3SO0kApEm2FMYb1xDrarE1zjS0mVZuaIDipkvW9P+7X7Xg5Mffec+49OX6eDzjc8/1+P/fe98cLeeac742pKiRJ7XrbpAeQJE2WIZCkxhkCSWqcIZCkxhkCSWrczKQHWI7zzz+/ZmdnJz2GJE2VAwcO/LqqNi48P5UhmJ2dpd/vT3oMSZoqSX4x7LxvDUlS4wyBJDXOEEhS4wyBJDXOEEhS4wyBJDXOEEhS4wyBJDXOEEhS4wyBJDXOEEhS4wyBJDXOEEhS4wyBJDXOEEhS4wyBJDXOEEhS4wyBJDXOEEhS4wyBJDXOEEhS4wyBJDXOEEhS4wyBJDXOEEhS48YSgiTbkhxKMpdk15Dra5M80F1/PMnsguubk7yc5NPjmEeStHgjhyDJGuCrwI3AFuCjSbYsWPZx4GRVXQ7cB9yz4PrfA98ddRZJ0tKN4xXBVcBcVR2pqteAvcD2BWu2A3u65w8B1yYJQJKbgGeAg2OYRZK0ROMIwSXAswPHR7tzQ9dU1SngJeC8JOcAnwE+d6ZvkmRnkn6S/vHjx8cwtiQJJn+z+E7gvqp6+UwLq2p3VfWqqrdx48aVn0ySGjEzhq9xDLh04HhTd27YmqNJZoBzgReBq4Gbk9wLrAN+m+Q3VfWVMcwlSVqEcYTgCeCKJJcx/wf+LcCfLVizD9gB/AdwM/C9qirgj19fkORO4GUjIEmra+QQVNWpJLcCDwNrgG9U1cEkdwH9qtoHfB24P8kccIL5WEiSzgKZ/4v5dOn1etXv9yc9hiRNlSQHqqq38PykbxZLkibMEEhS4wyBJDXOEEhS4wyBJDXOEEhS4wyBJDXOEEhS4wyBJDXOEEhS4wyBJDXOEEhS4wyBJDXOEEhS4wyBJDXOEEhS4wyBJDXOEEhS4wyBJDXOEEhS4wyBJDXOEEhS4wyBJDXOEEhS4wyBJDXOEEhS4wyBJDXOEEhS4wyBJDXOEEhS48YSgiTbkhxKMpdk15Dra5M80F1/PMlsd/76JAeS/Kj7+IFxzCNJWryRQ5BkDfBV4EZgC/DRJFsWLPs4cLKqLgfuA+7pzv8a+GBV/QGwA7h/1HkkSUszjlcEVwFzVXWkql4D9gLbF6zZDuzpnj8EXJskVfXDqvpld/4g8I4ka8cwkyRpkcYRgkuAZweOj3bnhq6pqlPAS8B5C9Z8BHiyql4dw0ySpEWamfQAAEmuZP7toq1vsmYnsBNg8+bNqzSZJL31jeMVwTHg0oHjTd25oWuSzADnAi92x5uAbwEfq6qnT/dNqmp3VfWqqrdx48YxjC1JgvGE4AngiiSXJXk7cAuwb8GafczfDAa4GfheVVWSdcB3gF1V9e9jmEWStEQjh6B7z/9W4GHgJ8CDVXUwyV1JPtQt+zpwXpI54Dbg9V8xvRW4HPibJE91jwtGnUmStHipqknPsGS9Xq/6/f6kx5CkqZLkQFX1Fp73XxZLUuMMgSQ1zhBIUuMMgSQ1zhBIUuMMgSQ1zhBIUuMMgSQ1zhBIUuMMgSQ1zhBIUuMMgSQ1zhBIUuMMgSQ1zhBIUuMMgSQ1zhBIUuMMgSQ1zhBIUuMMgSQ1zhBIUuMMgSQ1zhBIUuMMgSQ1zhBIUuMMgSQ1zhBIUuMMgSQ1zhBIUuMMgSQ1zhBIUuPGEoIk25IcSjKXZNeQ62uTPNBdfzzJ7MC1z3bnDyW5YRzzSJIWb+QQJFkDfBW4EdgCfDTJlgXLPg6crKrLgfuAe7rP3QLcAlwJbAP+oft6kqRVMo5XBFcBc1V1pKpeA/YC2xes2Q7s6Z4/BFybJN35vVX1alU9A8x1X0+StErGEYJLgGcHjo9254auqapTwEvAeYv8XACS7EzST9I/fvz4GMaWJMEU3Syuqt1V1auq3saNGyc9jiS9ZYwjBMeASweON3Xnhq5JMgOcC7y4yM+VJK2gcYTgCeCKJJcleTvzN3/3LVizD9jRPb8Z+F5VVXf+lu63ii4DrgB+MIaZJEmLNDPqF6iqU0luBR4G1gDfqKqDSe4C+lW1D/g6cH+SOeAE87GgW/cg8F/AKeCTVfU/o84kSVq8zP/FfLr0er3q9/uTHkOSpkqSA1XVW3h+am4WS5JWhiGQpMYZAklqnCGQpMYZAklqnCGQpMYZAklqnCGQpMYZAklqnCGQpMYZAklqnCGQpMYZAklqnCGQpMYZAklqnCGQpMYZAklqnCGQpMYZAklqnCGQpMYZAklqnCGQpMYZAklqnCGQpMYZAklqnCGQpMYZAklqnCGQpMYZAklqnCGQpMaNFIIkG5LsT3K4+7j+NOt2dGsOJ9nRnXtnku8k+WmSg0nuHmUWSdLyjPqKYBfwaFVdATzaHb9Bkg3AHcDVwFXAHQPB+Luq+j3gfcAfJrlxxHkkSUs0agi2A3u653uAm4asuQHYX1UnquoksB/YVlWvVNX3AarqNeBJYNOI80iSlmjUEFxYVc91z58HLhyy5hLg2YHjo925/5NkHfBB5l9VSJJW0cyZFiR5BLhoyKXbBw+qqpLUUgdIMgN8E/hyVR15k3U7gZ0AmzdvXuq3kSSdxhlDUFXXne5akheSXFxVzyW5GPjVkGXHgGsGjjcBjw0c7wYOV9WXzjDH7m4tvV5vycGRJA036ltD+4Ad3fMdwLeHrHkY2JpkfXeTeGt3jiRfAM4F/mrEOSRJyzRqCO4Grk9yGLiuOyZJL8nXAKrqBPB54InucVdVnUiyifm3l7YATyZ5KsknRpxHkrREqZq+d1l6vV71+/1JjyFJUyXJgarqLTzvvyyWpMYZAklqnCGQpMYZAklqnCGQpMYZAklqnCGQpMYZAklqnCGQpMYZAklqnCGQpMYZAklqnCGQpMYZAklqnCGQpMYZAklqnCGQpMYZAklqnCGQpMYZAklqnCGQpMYZAklqnCGQpMYZAklqnCGQpMYZAklqnCGQpMYZAklqnCGQpMYZAklqnCGQpMaNFIIkG5LsT3K4+7j+NOt2dGsOJ9kx5Pq+JD8eZRZJ0vKM+opgF/BoVV0BPNodv0GSDcAdwNXAVcAdg8FI8mHg5RHnkCQt06gh2A7s6Z7vAW4asuYGYH9Vnaiqk8B+YBtAknOA24AvjDiHJGmZRg3BhVX1XPf8eeDCIWsuAZ4dOD7anQP4PPBF4JUzfaMkO5P0k/SPHz8+wsiSpEEzZ1qQ5BHgoiGXbh88qKpKUov9xkneC7ynqj6VZPZM66tqN7AboNfrLfr7SJLe3BlDUFXXne5akheSXFxVzyW5GPjVkGXHgGsGjjcBjwHvB3pJft7NcUGSx6rqGiRJq2bUt4b2Aa//FtAO4NtD1jwMbE2yvrtJvBV4uKr+sareVVWzwB8BPzMCkrT6Rg3B3cD1SQ4D13XHJOkl+RpAVZ1g/l7AE93jru6cJOkskKrpe7u91+tVv9+f9BiSNFWSHKiq3sLz/stiSWqcIZCkxhkCSWqcIZCkxhkCSWqcIZCkxhkCSWqcIZCkxhkCSWqcIZCkxhkCSWqcIZCkxhkCSWqcIZCkxhkCSWqcIZCkxhkCSWqcIZCkxhkCSWqcIZCkxhkCSWqcIZCkxhkCSWqcIZCkxqWqJj3DkiU5Dvxi0nMs0fnAryc9xCpzz21wz9Pjd6tq48KTUxmCaZSkX1W9Sc+xmtxzG9zz9POtIUlqnCGQpMYZgtWze9IDTIB7boN7nnLeI5CkxvmKQJIaZwgkqXGGYIySbEiyP8nh7uP606zb0a05nGTHkOv7kvx45Sce3Sh7TvLOJN9J8tMkB5PcvbrTL02SbUkOJZlLsmvI9bVJHuiuP55kduDaZ7vzh5LcsJpzj2K5e05yfZIDSX7UffzAas++HKP8jLvrm5O8nOTTqzXzWFSVjzE9gHuBXd3zXcA9Q9ZsAI50H9d3z9cPXP8w8M/Ajye9n5XeM/BO4E+6NW8H/g24cdJ7Os0+1wBPA+/uZv1PYMuCNX8J/FP3/Bbgge75lm79WuCy7uusmfSeVnjP7wPe1T3/feDYpPezkvsduP4Q8C/Apye9n6U8fEUwXtuBPd3zPcBNQ9bcAOyvqhNVdRLYD2wDSHIOcBvwhVWYdVyWveeqeqWqvg9QVa8BTwKbVmHm5bgKmKuqI92se5nf+6DB/xYPAdcmSXd+b1W9WlXPAHPd1zvbLXvPVfXDqvpld/4g8I4ka1dl6uUb5WdMkpuAZ5jf71QxBON1YVU91z1/HrhwyJpLgGcHjo925wA+D3wReGXFJhy/UfcMQJJ1wAeBR1diyDE44x4G11TVKeAl4LxFfu7ZaJQ9D/oI8GRVvbpCc47Lsvfb/SXuM8DnVmHOsZuZ9ADTJskjwEVDLt0+eFBVlWTRv5ub5L3Ae6rqUwvfd5y0ldrzwNefAb4JfLmqjixvSp2NklwJ3ANsnfQsK+xO4L6qerl7gTBVDMESVdV1p7uW5IUkF1fVc0kuBn41ZNkx4JqB403AY8D7gV6SnzP/c7kgyWNVdQ0TtoJ7ft1u4HBVfWkM466UY8ClA8ebunPD1hzt4nYu8OIiP/dsNMqeSbIJ+Bbwsap6euXHHdko+70auDnJvcA64LdJflNVX1n5scdg0jcp3koP4G95443Te4es2cD8+4jru8czwIYFa2aZnpvFI+2Z+fsh/wq8bdJ7OcM+Z5i/yX0Z/38j8coFaz7JG28kPtg9v5I33iw+wnTcLB5lz+u69R+e9D5WY78L1tzJlN0snvgAb6UH8++NPgocBh4Z+MOuB3xtYN1fMH/DcA748yFfZ5pCsOw9M/83rgJ+AjzVPT4x6T29yV7/FPgZ879Zcnt37i7gQ93z32H+N0bmgB8A7x743Nu7zzvEWfqbUePcM/DXwH8P/FyfAi6Y9H5W8mc88DWmLgT+LyYkqXH+1pAkNc4QSFLjDIEkNc4QSFLjDIEkNc4QSFLjDIEkNe5/AecL/ch2b2HBAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "def create_binary_data(size):\n",
    "    input = []\n",
    "    output = []\n",
    "    \n",
    "    for _ in range(size):\n",
    "        tmp = []\n",
    "        for __ in range(16):\n",
    "            tmp.append((_ >> __) &1)\n",
    "        input.append(tmp)\n",
    "        output.append([_])\n",
    "    return input, output\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    neural = NeuralNetwork(\n",
    "        {\n",
    "            \"size\": [16, 10, 1],\n",
    "            \"activation\": [\"input\", \"sigmoid\", \"sigmoid\" ],\n",
    "        }\n",
    "    );\n",
    "    \n",
    "    minput, moutput = create_binary_data(15)\n",
    "\n",
    "    moutput = np.array(moutput, dtype=np.float64)\n",
    "    moutput = moutput / 1e+10\n",
    "    \n",
    "    epoch = 15000\n",
    "    batch = 4\n",
    "    \n",
    "    neural.train(np.array(minput, dtype=np.float64).T, np.array(moutput, dtype=np.float64).T, epoch)\n",
    "\n",
    "    plt.figure(1)\n",
    "    plt.plot(range(0, epoch), neural.losses)\n",
    "    plt.subplot()\n",
    "    plt.plot(range(0, epoch), neural.cost)\n",
    "    plt.figure(2)\n",
    "    plt.plot(range(0, epoch), neural.accurency)\n",
    "    plt.figure(3)\n",
    "    \n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
